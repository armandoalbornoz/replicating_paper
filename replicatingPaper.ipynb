{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO5eceQM3MXwnvPOH9a0hCi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/armandoalbornoz/replicating_paper/blob/main/replicatingPaper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wBo76l8VIVIL",
        "outputId": "13e7263e-6081-4720-fcd7-43da3fb87048"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GitHub Username: armandoalbornoz\n",
            "GitHub Email: albornoz.armando31416@gmail.com\n",
            "GitHub Personal Access Token: ··········\n",
            "Cloning into 'replicating_paper'...\n",
            "remote: Enumerating objects: 3, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 3 (delta 0), reused 3 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (3/3), done.\n",
            "/content/replicating_paper\n",
            "On branch main\n",
            "Your branch is up to date with 'origin/main'.\n",
            "\n",
            "nothing to commit, working tree clean\n",
            "Branch 'main' set up to track remote branch 'main' from 'origin'.\n",
            "Everything up-to-date\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "repo_name = \"replicating_paper\"\n",
        "repo_path = f\"/content/{repo_name}\"\n",
        "\n",
        "if not os.path.exists(repo_path):\n",
        "    # Securely input GitHub credentials\n",
        "    username = input(\"GitHub Username: \")\n",
        "    email = input(\"GitHub Email: \")\n",
        "    token = getpass.getpass(\"GitHub Personal Access Token: \")\n",
        "\n",
        "    # Clone the empty GitHub repo\n",
        "    !git clone https://{username}:{token}@github.com/{username}/{repo_name}.git\n",
        "    %cd {repo_name}\n",
        "\n",
        "    # Set Git identity securely\n",
        "    !git config user.email \"{email}\"\n",
        "    !git config user.name \"{username}\"\n",
        "\n",
        "    # Create a dummy file\n",
        "    !echo \"# Initial Commit\" > README.md\n",
        "\n",
        "    # Commit and push safely\n",
        "    !git add .\n",
        "    !git commit -m \"Initial commit\"\n",
        "    !git branch -M main\n",
        "    !git push -u origin main\n",
        "\n",
        "    # Remove tokenized remote URL for safety\n",
        "    !git remote set-url origin https://github.com/{username}/{repo_name}.git\n",
        "\n",
        "else:\n",
        "    print(f\"[INFO] Repo folder '{repo_name}' already exists. Skipping setup.\")\n",
        "    %cd {repo_path}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PyTorch Paper Replication\n",
        "\n",
        "In this notebook we will replicate the Vision Transformer Architecture (ViT):\n",
        "[paper](https://arxiv.org/pdf/2010.11929)"
      ],
      "metadata": {
        "id": "tidoAO7tK_h1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "from torch import nn\n",
        "from torchvision import transforms\n",
        "\n",
        "# Get torchinfo\n",
        "try:\n",
        "    from torchinfo import summary\n",
        "except:\n",
        "    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n",
        "    !pip install -q torchinfo\n",
        "    from torchinfo import summary\n",
        "\n",
        "# Get some helpful Python scripts from the following repo https://github.com/mrdbourke/pytorch-deep-learning\n",
        "try:\n",
        "    from going_modular.going_modular import data_setup, engine\n",
        "    from helper_functions import download_data, set_seeds, plot_loss_curves\n",
        "except:\n",
        "    # Get the going_modular scripts\n",
        "    print(\"[INFO] Couldn't find the scripts directory... downloading it from GitHub.\")\n",
        "    !git clone https://github.com/mrdbourke/pytorch-deep-learning\n",
        "    !mv pytorch-deep-learning/going_modular .\n",
        "    !mv pytorch-deep-learning/helper_functions.py . # get the helper_functions.py script\n",
        "    !rm -rf pytorch-deep-learning\n",
        "    from going_modular.going_modular import data_setup, engine\n",
        "    from helper_functions import download_data, set_seeds, plot_loss_curves"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GsuynM16PpoC",
        "outputId": "db4a833a-0725-44d3-931a-9be1867c1819"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Couldn't find torchinfo... installing it.\n",
            "[INFO] Couldn't find the scripts directory... downloading it from GitHub.\n",
            "Cloning into 'pytorch-deep-learning'...\n",
            "remote: Enumerating objects: 4393, done.\u001b[K\n",
            "remote: Counting objects: 100% (1534/1534), done.\u001b[K\n",
            "remote: Compressing objects: 100% (135/135), done.\u001b[K\n",
            "remote: Total 4393 (delta 1457), reused 1399 (delta 1399), pack-reused 2859 (from 2)\u001b[K\n",
            "Receiving objects: 100% (4393/4393), 650.71 MiB | 33.14 MiB/s, done.\n",
            "Resolving deltas: 100% (2659/2659), done.\n",
            "Updating files: 100% (248/248), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's setup device agnostice code\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "lQASJdCmQTwu",
        "outputId": "ca1c7ff7-8ee4-4bb4-b814-9d7561e14f1a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cpu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get data_setup\n",
        "\n",
        "We want to replicate the ViT architecture to apply it to a food classification problem, where we classify images of pizza, sushi and steak"
      ],
      "metadata": {
        "id": "MfHXIZ4YUBHM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download pizza, steak, sushi images from GitHub\n",
        "\n",
        "image_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n",
        "                           destination=\"pizza_steak_sushi\")\n",
        "image_path\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lGu7rLskVd5a",
        "outputId": "ccac60f7-022b-4439-ddb9-7194f639de1f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Did not find data/pizza_steak_sushi directory, creating one...\n",
            "[INFO] Downloading pizza_steak_sushi.zip from https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip...\n",
            "[INFO] Unzipping pizza_steak_sushi.zip data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PosixPath('data/pizza_steak_sushi')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir = image_path / \"train\"\n",
        "test_dir = image_path / \"test\""
      ],
      "metadata": {
        "id": "MlRXQi4FWGqo"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir, test_dir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67XtUr7QWbOC",
        "outputId": "896a6c85-b086-442a-db15-d5e001fdea0b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(PosixPath('data/pizza_steak_sushi/train'),\n",
              " PosixPath('data/pizza_steak_sushi/test'))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's turn the data into Datasets and DataLoaders\n",
        "\n",
        "from torchvision import transforms\n",
        "from going_modular.going_modular import data_setup\n",
        "\n",
        "IMG_SIZE = 224 # Found in the paper\n",
        "BATCH_SIZE = 32 # The paper uses 4096 but we make it smaller due to hardware limitations\n",
        "\n",
        "manual_transforms = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "train_dataloader, test_dataloader , class_names = data_setup.create_dataloaders(train_dir=train_dir,\n",
        "                                                                                test_dir=test_dir,\n",
        "                                                                                transform=manual_transforms,\n",
        "                                                                                batch_size=BATCH_SIZE,\n",
        "                                                                                num_workers=os.cpu_count())"
      ],
      "metadata": {
        "id": "TEdzDd18WhCP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Replicating ViT\n",
        "\n",
        "We need to break down what we are building into its building blocks:\n",
        "\n",
        "* **Inputs** Image tensors,\n",
        "* **Outputs**: Image classification Labels\n",
        "* **Layers**: Self-Attention,\n",
        "* **Blocks**: Collections of layers,\n",
        "* **Model**: A collection of blocks"
      ],
      "metadata": {
        "id": "LtvUMt1OY9iN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ViT Overview\n",
        "\n",
        "### Visual Overview\n",
        "![](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/images/08-vit-paper-figure-1-architecture-overview.png?raw=true)\n",
        "\n",
        "\n",
        "### Required Equations\n",
        "\n",
        "![](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/images/08-vit-paper-four-equations.png?raw=true)\n",
        "\n",
        "### Table 1\n",
        "\n",
        "![](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/images/08-vit-paper-table-1.png?raw=true)\n",
        "\n",
        "\n",
        "### This Section describes various equations\n",
        "\n",
        "The standard Transtormer receives as input a 1D sequence of token embeddings. To handle 2D images, we reshape the image $\\mathrm{x} \\in \\mathbb{R}^{H \\times W \\times C}$ into a sequence of flattened 2D patches $\\mathbf{x}_p \\in \\mathbb{R}^{N \\times\\left(P^2 \\cdot C\\right)}$, where $(H, W)$ is the resolution of the original image, $C$ is the number of channels, $(P, P)$ is the resolution of each image patch, and $N=H W / P^2$ is the resulting number of patches, which also serves as the effective input sequence length for the Transformer. The Transformer uses constant latent vector size $D$ through all of its layers, so we flatten the patches and map to $D$ dimensions with a trainable linear projection (Eq. 1 ). We refer to the output of this projection as the patch embeddings.\n",
        "\n",
        "\n",
        "Similar to BERT's [class] token, we prepend a learnable embedding to the sequence of embedded patches $\\left(z_0^0=x_{\\text {class }}\\right)$, whose state at the output of the Transformer encoder $\\left(z_L^0\\right)$ serves as the image representation $y$ (Eq. 4). Both during pre-training and fine-tuning, a classification head is attached to $\\mathbf{z}_L^0$. The classification head is implemented by a MLP with one hidden layer at pre-training time and by a single linear layer at fine-tuning time.\n",
        "\n",
        "Position embeddings are added to the patch embeddings to retain positional information. We use standard learnable 1D position embeddings, since we have not observed significant performance gains from using more advanced 2D-aware position embeddings (Appendix D.4). The resulting sequence of embedding vectors serves as input to the encoder.\n",
        "The Transformer encoder (Vaswani et al., 2017) consists of alternating layers of multiheaded selfattention (MSA, see Appendix A) and MLP blocks (Eq. 2, 3). Layernorm (LN) is applied before every block, and residual connections after every block (Wang et al., 2019; Baevski \\& Auli, 2019).\n",
        "\n",
        "Let's now translate these equations to pseudocode\n",
        "\n",
        "**Equation 1**:\n",
        "\n",
        "```python\n",
        "x_input = [class_token, image_patch_1, image_patch_2, ..., image_patch_N] +\n",
        "          [class_token_pos, image_patch_1_pos, ..., image_patch_N_pos]\n",
        "```\n",
        "\n",
        "** Equation 2**:\n",
        "\n",
        "```python\n",
        "x_output_MSA_block = MSA_layer(LN_layer(x_input)) + x_input\n",
        "```\n",
        "\n",
        "** Equation 3**:\n",
        "\n",
        "```python\n",
        "x_output_MLP_block = MLP_layer(LN_layer(x_output_MSA_block)) + x_output_MSA_block\n",
        "```\n",
        "\n",
        "\n",
        "** Equation 4**:\n",
        "\n",
        "```python\n",
        "y  = Linear_layer(LN_layer(x_output_MLP_block))\n",
        "```\n"
      ],
      "metadata": {
        "id": "zXRK509SZIDP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git status"
      ],
      "metadata": {
        "id": "BMZLA0lSfi0m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7552a417-24b4-4823-ee23-76e41f98e5eb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "On branch main\n",
            "Your branch is up to date with 'origin/main'.\n",
            "\n",
            "Untracked files:\n",
            "  (use \"git add <file>...\" to include in what will be committed)\n",
            "\t\u001b[31m__pycache__/\u001b[m\n",
            "\t\u001b[31mdata/\u001b[m\n",
            "\t\u001b[31mgoing_modular/\u001b[m\n",
            "\t\u001b[31mhelper_functions.py\u001b[m\n",
            "\n",
            "nothing added to commit but untracked files present (use \"git add\" to track)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kYguDBmtmfWz"
      },
      "execution_count": 8,
      "outputs": []
    }
  ]
}